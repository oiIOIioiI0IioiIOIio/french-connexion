import sys
import os
import wikipedia
import yaml
import frontmatter
from pathlib import Path
from dotenv import load_dotenv
import json
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict
import time

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.utils.logger import setup_logger
from src.utils.git_handler import GitHandler
from src.utils.llm_client import MistralClient

# Configuration
load_dotenv()
logger = setup_logger()
git = GitHandler()
llm = MistralClient()

# Wikipedia en fran√ßais
wikipedia.set_lang("fr")

# Chargement config
with open("config/config.yaml", "r", encoding="utf-8") as f:
    CONFIG = yaml.safe_load(f)

# Variables globales pour tracker l'exploration
VISITED_PEOPLE = set()
VISITED_ORGS = set()
ALL_FOUND_ENTITIES = []
EXPLORATION_STATS = defaultdict(int)
RELATIONSHIPS_GRAPH = defaultdict(list)
VALIDATION_SCORES = {}
CREATED_FILES = []
ORIGINAL_QUERY = ""

# Configuration de l'exploration
MAX_DEPTH = 3
CONFIDENCE_THRESHOLD = 0.6  # Score minimum pour validation
EXPONENTIAL_EXPLORATION = True  # Exploration compl√®te sans limite

# Structures de retour par d√©faut pour les erreurs
EMPTY_ENTITY_RESPONSE = {
    'people': [],
    'institutions': [],
    'main_subject': '',
    'subject_type': 'unknown',
    'description': '',
    'keywords': [],
    'context': '',
    'relevance_explanation': ''
}

EMPTY_QUERY_RESPONSE = {
    'query_type': 'unknown',
    'people': [],
    'institutions': [],
    'interpretation': '',
    'main_subject': '',
    'subject_category': '',
    'explanation': ''
}

class PersonEntity:
    """Classe pour repr√©senter une personne avec toutes ses m√©tadonn√©es"""
    
    def __init__(self, name: str, depth: int, found_via: str, query: str):
        self.name = name
        self.depth = depth
        self.found_via = found_via
        self.original_query = query
        self.wikipedia_data = None
        self.validation_score = 0.0
        self.validation_reason = ""
        self.is_validated = False
        self.relationships = []
        self.organizations = []
        self.created_file_path = None
        self.factcheck_status = "pending"
        self.sources = []
        
    def to_dict(self) -> dict:
        """Convertit l'entit√© en dictionnaire"""
        return {
            'name': self.name,
            'depth': self.depth,
            'found_via': self.found_via,
            'original_query': self.original_query,
            'validation_score': self.validation_score,
            'validation_reason': self.validation_reason,
            'is_validated': self.is_validated,
            'factcheck_status': self.factcheck_status,
            'relationships_count': len(self.relationships),
            'organizations_count': len(self.organizations)
        }

class InstitutionEntity:
    """Classe pour repr√©senter une institution"""
    
    def __init__(self, name: str, depth: int, found_via: str):
        self.name = name
        self.depth = depth
        self.found_via = found_via
        self.wikipedia_data = None
        self.members = []
        self.created_file_path = None
        self.factcheck_status = "pending"
        
    def to_dict(self) -> dict:
        return {
            'name': self.name,
            'depth': self.depth,
            'found_via': self.found_via,
            'factcheck_status': self.factcheck_status,
            'members_count': len(self.members)
        }

class RelationshipDetail:
    """Classe pour repr√©senter une relation d√©taill√©e entre deux personnes"""
    
    def __init__(self, person_from: str, person_to: str, relationship_type: str, 
                 description: str, confidence: float, source: str):
        self.person_from = person_from
        self.person_to = person_to
        self.relationship_type = relationship_type
        self.description = description
        self.confidence = confidence
        self.source = source
        self.timestamp = datetime.now()
        
    def to_markdown(self) -> str:
        """Convertit la relation en format Markdown pour Obsidian"""
        return f"- [[{self.person_to}]] : {self.description} ({self.relationship_type})"
    
    def to_dict(self) -> dict:
        return {
            'person_from': self.person_from,
            'person_to': self.person_to,
            'type': self.relationship_type,
            'description': self.description,
            'confidence': self.confidence,
            'source': self.source
        }

def mistral_identify_entities_comprehensive(query: str, context: str = "", query_type_hint: str = None) -> dict:
    """
    üß† Identification compl√®te des entit√©s via Mistral
    Utilise la connaissance g√©n√©rale pour identifier personnes et institutions
    """
    logger.info(f"üß† Identification compl√®te des entit√©s pour : {query}")
    
    context_text = f"\n\nCONTEXTE ADDITIONNEL :\n{context}" if context else ""
    type_hint = f"\n\nHINT : Cette requ√™te est de type '{query_type_hint}'" if query_type_hint else ""
    
    prompt = f"""
Tu es un expert mondial des r√©seaux de pouvoir, √©lites, institutions et g√©opolitique.

REQU√äTE : "{query}"{context_text}{type_hint}

Ta mission : identifier de mani√®re EXHAUSTIVE et RIGOUREUSE toutes les personnes et institutions 
pertinentes, en utilisant ta connaissance g√©n√©rale (niveau journalistique).

‚ö†Ô∏è R√àGLE CRITIQUE : 
- Si la requ√™te contient "dirigeants", "membres", "pr√©sidents", "ministres" ‚Üí ce sont des PERSONNES
- JAMAIS traiter un groupe de personnes comme une institution
- "dirigeants de X" = personnes, pas institution

EXEMPLES D√âTAILL√âS :

Requ√™te "les dirigeants de LVMH" ‚Üí
{{
  "main_subject": "LVMH",
  "subject_type": "people_group",
  "description": "Dirigeants et cadres ex√©cutifs du groupe LVMH",
  "people": [
    "Bernard Arnault",
    "Antoine Arnault",
    "Delphine Arnault",
    "Sidney Toledano",
    "Pietro Beccari",
    "Michael Burke",
    "Jean-Jacques Guiony"
  ],
  "institutions": ["LVMH", "Christian Dior", "Louis Vuitton"],
  "keywords": ["luxe", "dirigeant", "entreprise", "famille Arnault"],
  "context": "Direction du premier groupe de luxe mondial",
  "relevance_explanation": "Cadres dirigeants de LVMH - ce sont des PERSONNES occupant des fonctions de direction"
}}

Requ√™te "Le Si√®cle" ‚Üí
{{
  "main_subject": "Le Si√®cle",
  "subject_type": "institution",
  "description": "Club de r√©flexion fran√ßais fond√© en 1944, r√©unissant √©lites politiques, √©conomiques et m√©diatiques",
  "people": [
    "Henri de Castries",
    "Anne Lauvergeon", 
    "Nicole Notat",
    "Thierry Breton",
    "Laurence Parisot",
    "Jean-Marie Colombani",
    "Christine Lagarde",
    "Fran√ßois P√©rol",
    "Bernard Arnault"
  ],
  "institutions": ["Le Si√®cle", "MEDEF", "Institut Montaigne", "ENA"],
  "keywords": ["√©lite", "r√©seau", "influence", "club", "pouvoir"],
  "context": "R√©seau d'influence fran√ßais majeur depuis 1944",
  "relevance_explanation": "Club priv√© rassemblant les principales √©lites fran√ßaises"
}}

Requ√™te "Jeffrey Epstein" ‚Üí
{{
  "main_subject": "Jeffrey Epstein",
  "subject_type": "personne",
  "description": "Financier am√©ricain condamn√© pour trafic de mineurs et crimes sexuels, d√©c√©d√© en prison en 2019",
  "people": [
    "Jeffrey Epstein",
    "Ghislaine Maxwell",
    "Les Wexner",
    "Bill Clinton",
    "Donald Trump",
    "Prince Andrew",
    "Alan Dershowitz",
    "Jean-Luc Brunel"
  ],
  "institutions": ["Victoria's Secret", "L Brands", "MIT Media Lab", "Council on Foreign Relations"],
  "keywords": ["finance", "scandale", "trafic", "√©lite", "connexions"],
  "context": "R√©seau de trafic de mineurs impliquant des personnalit√©s internationales",
  "relevance_explanation": "Affaire criminelle majeure r√©v√©lant des connexions au sein des √©lites"
}}

R√àGLES STRICTES :
1. Utilise ta CONNAISSANCE G√âN√âRALE (sources fiables uniquement)
2. Liste TOUTES les personnes pertinentes (10-30 personnes selon le sujet)
3. Liste TOUTES les institutions/organisations pertinentes
4. main_subject = le sujet EXACT de la requ√™te
5. subject_type = "personne", "people_group", ou "institution"
   - "people_group" = requ√™te demandant un groupe de personnes (dirigeants, membres, etc.)
   - JAMAIS "institution" pour des groupes de personnes
6. Sois EXHAUSTIF mais RIGOUREUX
7. N'invente AUCUNE information
8. Privil√©gie les personnes DOCUMENT√âES et V√âRIFIABLES

Retourne un JSON complet :
"""
    
    try:
        chat_response = llm.client.chat.complete(
            model=llm.model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3  # Plus bas pour plus de fiabilit√©
        )
        
        if chat_response.choices and chat_response.choices[0].message:
            result = json.loads(chat_response.choices[0].message.content)
            
            EXPLORATION_STATS['mistral_calls'] += 1
            EXPLORATION_STATS['entities_identified'] += len(result.get('people', []))
            EXPLORATION_STATS['institutions_identified'] += len(result.get('institutions', []))
            
            logger.info(f"‚úÖ Sujet principal : {result.get('main_subject', 'N/A')} (type: {result.get('subject_type', 'N/A')})")
            logger.info(f"‚úÖ {len(result.get('people', []))} personnes identifi√©es")
            logger.info(f"‚úÖ {len(result.get('institutions', []))} institutions identifi√©es")
            
            return result
        
        return EMPTY_ENTITY_RESPONSE.copy()
        
    except Exception as e:
        logger.error(f"‚ùå Erreur Mistral identification : {e}")
        EXPLORATION_STATS['errors'] += 1
        return EMPTY_ENTITY_RESPONSE.copy()

def answer_initial_query_directly(query: str) -> dict:
    """
    üéØ R√âPOND DIRECTEMENT √† la requ√™te initiale AVANT l'exploration r√©cursive
    Distingue les requ√™tes sur des GROUPES DE PERSONNES vs des INSTITUTIONS
    """
    logger.info(f"üéØ R√©ponse directe √† la requ√™te : {query}")
    
    prompt = f"""
Tu es un expert en analyse de requ√™tes et identification d'entit√©s.

REQU√äTE : "{query}"

Ta mission : d√©terminer si cette requ√™te demande des PERSONNES ou une INSTITUTION, puis r√©pondre DIRECTEMENT.

R√àGLES DE CLASSIFICATION STRICTES :

1. REQU√äTE SUR DES PERSONNES (liste de personnes) :
   - Contient : "dirigeants", "membres", "pr√©sidents", "ministres", "personnes", "qui sont", etc.
   - Exemples : "les dirigeants de LVMH", "les membres du Si√®cle", "les pr√©sidents fran√ßais"
   - Type : "people_group"
   
2. REQU√äTE SUR UNE PERSONNE UNIQUE :
   - Nom propre d'une personne sp√©cifique
   - Exemples : "Emmanuel Macron", "Bernard Arnault", "Jeffrey Epstein"
   - Type : "single_person"
   
3. REQU√äTE SUR UNE INSTITUTION :
   - Nom d'organisation, entreprise, club, think tank
   - Exemples : "Le Si√®cle", "LVMH", "Groupe Bilderberg"
   - Type : "institution"

INSTRUCTIONS SELON LE TYPE :

Si type = "people_group" :
- Identifie l'organisation/contexte mentionn√©
- Liste TOUTES les personnes pertinentes (dirigeants, membres, etc.)
- Minimum 5-20 personnes selon le contexte

Si type = "single_person" :
- Identifie la personne
- Liste ses relations principales (5-15 personnes)

Si type = "institution" :
- Identifie l'institution
- Liste ses membres/dirigeants principaux (10-30 personnes)

EXEMPLES D√âTAILL√âS :

Requ√™te "les dirigeants de LVMH" ‚Üí
{{
  "query_type": "people_group",
  "main_subject": "LVMH",
  "subject_category": "entreprise",
  "interpretation": "Liste des dirigeants et cadres dirigeants de LVMH",
  "people": [
    "Bernard Arnault",
    "Antoine Arnault",
    "Delphine Arnault",
    "Sidney Toledano",
    "Pietro Beccari",
    "Michael Burke",
    "Jean-Jacques Guiony",
    "Chantal Gaemperle"
  ],
  "institutions": ["LVMH", "Christian Dior", "Louis Vuitton", "Mo√´t Hennessy"],
  "explanation": "Requ√™te demandant explicitement les DIRIGEANTS (personnes) de LVMH, pas l'entreprise elle-m√™me"
}}

Requ√™te "Le Si√®cle" ‚Üí
{{
  "query_type": "institution",
  "main_subject": "Le Si√®cle",
  "subject_category": "club d'influence",
  "interpretation": "Club r√©unissant les √©lites fran√ßaises - liste de ses membres",
  "people": [
    "Henri de Castries",
    "Anne Lauvergeon",
    "Nicole Notat",
    "Thierry Breton",
    "Christine Lagarde",
    "Bernard Arnault",
    "Fran√ßois P√©rol"
  ],
  "institutions": ["Le Si√®cle", "MEDEF", "Institut Montaigne"],
  "explanation": "Institution dont on veut conna√Ætre les membres"
}}

Requ√™te "Bernard Arnault" ‚Üí
{{
  "query_type": "single_person",
  "main_subject": "Bernard Arnault",
  "subject_category": "chef d'entreprise",
  "interpretation": "Personne sp√©cifique et son r√©seau",
  "people": [
    "Bernard Arnault",
    "Antoine Arnault",
    "Delphine Arnault",
    "Sidney Toledano",
    "Fran√ßois Pinault",
    "Emmanuel Macron"
  ],
  "institutions": ["LVMH", "Christian Dior", "Le Si√®cle"],
  "explanation": "Personne unique dont on explore le r√©seau"
}}

IMPORTANT :
- Si la requ√™te contient "dirigeants", "membres", "qui sont", "liste", etc. ‚Üí query_type = "people_group"
- TOUJOURS privil√©gier "people_group" en cas de doute avec des mots au pluriel
- Liste EXHAUSTIVE de personnes (utilise ta connaissance g√©n√©rale)

Retourne un JSON complet :
"""
    
    try:
        chat_response = llm.client.chat.complete(
            model=llm.model,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        
        if chat_response.choices and chat_response.choices[0].message:
            result = json.loads(chat_response.choices[0].message.content)
            
            query_type = result.get('query_type', 'unknown')
            people = result.get('people', [])
            institutions = result.get('institutions', [])
            interpretation = result.get('interpretation', '')
            
            logger.info(f"‚úÖ Type de requ√™te identifi√© : {query_type}")
            logger.info(f"‚úÖ Sujet principal : {result.get('main_subject', 'N/A')}")
            logger.info(f"‚úÖ Interpr√©tation : {interpretation}")
            logger.info(f"‚úÖ {len(people)} personnes identifi√©es directement")
            logger.info(f"‚úÖ {len(institutions)} institutions identifi√©es")
            
            return result
        
        return EMPTY_QUERY_RESPONSE.copy()
        
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©ponse directe : {e}")
        return EMPTY_QUERY_RESPONSE.copy()

def answer_initial_query_directly(query: str) -> dict:
    """
    üéØ R√âPOND DIRECTEMENT √† la requ√™te initiale AVANT l'exploration r√©cursive
    Distingue les requ√™tes sur des GROUPES DE PERSONNES vs des INSTITUTIONS
    """
    logger.info(f"üéØ R√©ponse directe √† la requ√™te : {query}")
    
    prompt = f"""
Tu es un expert en analyse de requ√™tes et identification d'entit√©s.

REQU√äTE : "{query}"

Ta mission : d√©terminer si cette requ√™te demande des PERSONNES ou une INSTITUTION, puis r√©pondre DIRECTEMENT.

R√àGLES DE CLASSIFICATION STRICTES :

1. REQU√äTE SUR DES PERSONNES (liste de personnes) :
   - Contient : "dirigeants", "membres", "pr√©sidents", "ministres", "personnes", "qui sont", etc.
   - Exemples : "les dirigeants de LVMH", "les membres du Si√®cle", "les pr√©sidents fran√ßais"
   - Type : "people_group"
   
2. REQU√äTE SUR UNE PERSONNE UNIQUE :
   - Nom propre d'une personne sp√©cifique
   - Exemples : "Emmanuel Macron", "Bernard Arnault", "Jeffrey Epstein"
   - Type : "single_person"
   
3. REQU√äTE SUR UNE INSTITUTION :
   - Nom d'organisation, entreprise, club, think tank
   - Exemples : "Le Si√®cle", "LVMH", "Groupe Bilderberg"
   - Type : "institution"

INSTRUCTIONS SELON LE TYPE :

Si type = "people_group" :
- Identifie l'organisation/contexte mentionn√©
- Liste TOUTES les personnes pertinentes (dirigeants, membres, etc.)
- Minimum 5-20 personnes selon le contexte

Si type = "single_person" :
- Identifie la personne
- Liste ses relations principales (5-15 personnes)

Si type = "institution" :
- Identifie l'institution
- Liste ses membres/dirigeants principaux (10-30 personnes)

EXEMPLES D√âTAILL√âS :

Requ√™te "les dirigeants de LVMH" ‚Üí
{{
  "query_type": "people_group",
  "main_subject": "LVMH",
  "subject_category": "entreprise",
  "interpretation": "Liste des dirigeants et cadres dirigeants de LVMH",
  "people": [
    "Bernard Arnault",
    "Antoine Arnault",
    "Delphine Arnault",
    "Sidney Toledano",
    "Pietro Beccari",
    "Michael Burke",
    "Jean-Jacques Guiony",
    "Chantal Gaemperle"
  ],
  "institutions": ["LVMH", "Christian Dior", "Louis Vuitton", "Mo√´t Hennessy"],
  "explanation": "Requ√™te demandant explicitement les DIRIGEANTS (personnes) de LVMH, pas l'entreprise elle-m√™me"
}}

Requ√™te "Le Si√®cle" ‚Üí
{{
  "query_type": "institution",
  "main_subject": "Le Si√®cle",
  "subject_category": "club d'influence",
  "interpretation": "Club r√©unissant les √©lites fran√ßaises - liste de ses membres",
  "people": [
    "Henri de Castries",
    "Anne Lauvergeon",
    "Nicole Notat",
    "Thierry Breton",
    "Christine Lagarde",
    "Bernard Arnault",
    "Fran√ßois P√©rol"
  ],
  "institutions": ["Le Si√®cle", "MEDEF", "Institut Montaigne"],
  "explanation": "Institution dont on veut conna√Ætre les membres"
}}

Requ√™te "Bernard Arnault" ‚Üí
{{
  "query_type": "single_person",
  "main_subject": "Bernard Arnault",
  "subject_category": "chef d'entreprise",
  "interpretation": "Personne sp√©cifique et son r√©seau",
  "people": [
    "Bernard Arnault",
    "Antoine Arnault",
    "Delphine Arnault",
    "Sidney Toledano",
    "Fran√ßois Pinault",
    "Emmanuel Macron"
  ],
  "institutions": ["LVMH", "Christian Dior", "Le Si√®cle"],
  "explanation": "Personne unique dont on explore le r√©seau"
}}

IMPORTANT :
- Si la requ√™te contient "dirigeants", "membres", "qui sont", "liste", etc. ‚Üí query_type = "people_group"
- TOUJOURS privil√©gier "people_group" en cas de doute avec des mots au pluriel
- Liste EXHAUSTIVE de personnes (utilise ta connaissance g√©n√©rale)

Retourne un JSON complet :
"""
    
    try:
        chat_response = llm.client.chat.complete(
            model=llm.model,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        
        if chat_response.choices and chat_response.choices[0].message:
            result = json.loads(chat_response.choices[0].message.content)
            
            query_type = result.get('query_type', 'unknown')
            people = result.get('people', [])
            institutions = result.get('institutions', [])
            interpretation = result.get('interpretation', '')
            
            logger.info(f"‚úÖ Type de requ√™te identifi√© : {query_type}")
            logger.info(f"‚úÖ Sujet principal : {result.get('main_subject', 'N/A')}")
            logger.info(f"‚úÖ Interpr√©tation : {interpretation}")
            logger.info(f"‚úÖ {len(people)} personnes identifi√©es directement")
            logger.info(f"‚úÖ {len(institutions)} institutions identifi√©es")
            
            return result
        
        return {}
        
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©ponse directe : {e}")
        return {}

def mistral_extract_detailed_relationships(person_name: str, bio_text: str, 
                                          all_known_people: Set[str]) -> List[RelationshipDetail]:
    """
    üîó Extraction D√âTAILL√âE des relations depuis une biographie Wikipedia
    Retourne des objets RelationshipDetail avec descriptions pr√©cises
    """
    logger.info(f"üîó Extraction d√©taill√©e des relations pour : {person_name}")
    
    known_people_list = list(all_known_people)[:50]  # Limiter pour le prompt
    
    prompt = f"""
Tu es un expert en analyse de r√©seaux et relations de pouvoir.

PERSONNE ANALYS√âE : {person_name}

BIOGRAPHIE WIKIPEDIA :
{bio_text}

PERSONNES D√âJ√Ä IDENTIFI√âES DANS LE R√âSEAU :
{', '.join(known_people_list)}

Ta mission : extraire TOUTES les relations significatives avec des descriptions PR√âCISES.

Pour chaque relation, identifie :
1. Le nom complet de la personne li√©e
2. Le type de relation (famille, collaborateur, mentor, associ√©, concurrent, etc.)
3. Une description FACTUELLE et PR√âCISE de la relation (1 phrase)
4. Un score de confiance (0.0 √† 1.0) bas√© sur la clart√© de l'information

EXEMPLES DE RELATIONS D√âTAILL√âES :

{{
  "relationships": [
    {{
      "person_name": "Ghislaine Maxwell",
      "relationship_type": "associ√©e",
      "description": "Associ√©e et compagne de longue date, impliqu√©e dans le r√©seau de trafic",
      "confidence": 0.95,
      "context": "Mentionn√©e 47 fois dans la biographie, relation document√©e sur 20 ans"
    }},
    {{
      "person_name": "Bill Clinton",
      "relationship_type": "relation professionnelle",
      "description": "A voyag√© √† plusieurs reprises dans l'avion priv√© d'Epstein entre 2002 et 2005",
      "confidence": 0.85,
      "context": "Relation document√©e par les logs de vol et t√©moignages"
    }},
    {{
      "person_name": "Les Wexner",
      "relationship_type": "mentor et associ√©",
      "description": "Principal client et mentor financier, PDG de L Brands, relation de 15 ans",
      "confidence": 0.90,
      "context": "Gestion de fortune et conseils financiers document√©s"
    }}
  ],
  "institutions": [
    {{
      "name": "Victoria's Secret",
      "relationship_type": "conseiller financier",
      "description": "Conseiller financier de Les Wexner, propri√©taire de la marque",
      "confidence": 0.80
    }}
  ]
}}

R√àGLES STRICTES :
1. N'extrais QUE les relations EXPLICITEMENT mentionn√©es dans le texte
2. Descriptions FACTUELLES uniquement (pas d'interpr√©tation)
3. Score de confiance bas√© sur la clart√© et la r√©p√©tition dans le texte
4. Maximum 20 relations (priorise les plus importantes)
5. Privil√©gie les personnes de la liste "PERSONNES D√âJ√Ä IDENTIFI√âES"
6. Aucune invention, aucune sp√©culation

Retourne un JSON :
"""
    
    try:
        chat_response = llm.client.chat.complete(
            model=llm.model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.2  # Tr√®s bas pour fiabilit√© maximale
        )
        
        if chat_response.choices and chat_response.choices[0].message:
            result = json.loads(chat_response.choices[0].message.content)
            relationships_data = result.get('relationships', [])
            
            relationships = []
            for rel in relationships_data:
                if rel.get('confidence', 0) >= 0.6:  # Seuil de confiance
                    relationship = RelationshipDetail(
                        person_from=person_name,
                        person_to=rel.get('person_name', ''),
                        relationship_type=rel.get('relationship_type', 'relation'),
                        description=rel.get('description', ''),
                        confidence=rel.get('confidence', 0.0),
                        source=f"Wikipedia - {person_name}"
                    )
                    relationships.append(relationship)
            
            EXPLORATION_STATS['relationships_extracted'] += len(relationships)
            logger.info(f"‚úÖ {len(relationships)} relations d√©taill√©es extraites (confiance ‚â• 0.6)")
            
            return relationships
        
        return []
        
    except Exception as e:
        logger.error(f"‚ùå Erreur extraction relations : {e}")
        EXPLORATION_STATS['errors'] += 1
        return []

def wikipedia_factcheck_person_rigorous(person_name: str) -> Optional[dict]:
    """
    üìñ Factchecking RIGOUREUX d'une personne via Wikipedia
    Niveau journalistique : v√©rification multiple, sources crois√©es
    """
    logger.info(f"üìñ Factcheck rigoureux pour : {person_name}")
    
    try:
        # Recherche Wikipedia
        page = wikipedia.page(person_name, auto_suggest=True)
        summary = page.summary
        full_content = page.content[:5000]  # Plus de contenu pour analyse
        
        logger.info(f"‚úÖ Page Wikipedia trouv√©e : {page.title}")
        
        # Sch√©ma d'extraction d√©taill√©
        schema = """
        {
          "nom_complet_verifie": "Nom complet exact selon Wikipedia",
          "date_naissance": "Date de naissance au format YYYY-MM-DD si disponible",
          "date_deces": "Date de d√©c√®s au format YYYY-MM-DD si applicable, sinon vide",
          "lieu_naissance": "Ville et pays de naissance complets",
          "nationalite": "Nationalit√©(s) compl√®te(s)",
          "genre": "homme ou femme",
          "statut_actuel": "Profession ou fonction principale actuelle ou au moment du d√©c√®s",
          "bio_courte": "R√©sum√© biographique factuel en 2-3 phrases maximum",
          "bio_detaillee": "Biographie d√©taill√©e en 5-7 phrases",
          "formation": "Liste compl√®te des √©coles, universit√©s, dipl√¥mes (format: liste)",
          "carriere": "Liste chronologique des principales fonctions, postes, mandats (format: liste)",
          "distinctions": "Liste compl√®te des distinctions, prix, d√©corations (format: liste)",
          "controverses": "Liste des controverses ou scandales document√©s (format: liste)",
          "famille_proche": "Noms complets des membres famille proche mentionn√©s (conjoint, enfants, parents)",
          "relations_professionnelles": "Noms complets des collaborateurs, mentors, associ√©s importants",
          "mots_cles": "Mots-cl√©s caract√©risant la personne (5-10 mots)",
          "niveau_notoriete": "Score de 1 √† 10 estimant la notori√©t√© publique",
          "sources_mentionnees": "Sources ou r√©f√©rences importantes mentionn√©es dans l'article"
        }
        """
        
        extracted_data = llm.extract_yaml_data(full_content, schema)
        
        # Normalisation des listes
        for key in ['formation', 'carriere', 'distinctions', 'controverses', 
                    'famille_proche', 'relations_professionnelles', 'mots_cles', 'sources_mentionnees']:
            if key not in extracted_data or extracted_data[key] is None:
                extracted_data[key] = []
            elif isinstance(extracted_data[key], str):
                items = [item.strip() for item in extracted_data[key].split(',') if item.strip()]
                extracted_data[key] = items
        
        # Enrichissement des donn√©es
        extracted_data['wikipedia_url'] = page.url
        extracted_data['wikipedia_title'] = page.title
        extracted_data['wikipedia_summary'] = summary[:500]
        extracted_data['verification_date'] = datetime.now().strftime('%Y-%m-%d')
        extracted_data['factcheck_status'] = 'verified'
        extracted_data['content_length'] = len(page.content)
        extracted_data['has_references'] = len(page.references) if hasattr(page, 'references') else 0
        
        # Extraction des relations d√©taill√©es
        all_known_people = VISITED_PEOPLE.copy()
        relationships = mistral_extract_detailed_relationships(person_name, full_content, all_known_people)
        
        extracted_data['detailed_relationships'] = relationships
        extracted_data['relationships_count'] = len(relationships)
        
        # Extraction des institutions li√©es
        institutions = extract_institutions_from_text(full_content)
        extracted_data['linked_institutions'] = institutions
        
        EXPLORATION_STATS['factcheck_success'] += 1
        logger.info(f"‚úÖ Factcheck r√©ussi : {page.title} ({len(relationships)} relations, {len(institutions)} institutions)")
        
        return extracted_data
        
    except wikipedia.exceptions.DisambiguationError as e:
        logger.warning(f"‚ö†Ô∏è  Ambigu√Øt√© pour {person_name}. Options : {e.options[:5]}")
        
        # Tentative avec la premi√®re option
        try:
            page = wikipedia.page(e.options[0])
            full_content = page.content[:5000]
            
            logger.info(f"‚úÖ Utilisation de la page : {page.title}")
            
            schema = """
            {
              "nom_complet_verifie": "Nom complet exact",
              "date_naissance": "Date de naissance",
              "date_deces": "Date de d√©c√®s si applicable",
              "lieu_naissance": "Lieu de naissance",
              "nationalite": "Nationalit√©",
              "genre": "Genre",
              "statut_actuel": "Statut professionnel",
              "bio_courte": "Biographie courte",
              "bio_detaillee": "Biographie d√©taill√©e",
              "formation": "Formation (liste)",
              "carriere": "Carri√®re (liste)",
              "distinctions": "Distinctions (liste)",
              "controverses": "Controverses (liste)",
              "mots_cles": "Mots-cl√©s",
              "niveau_notoriete": "Notori√©t√© (1-10)"
            }
            """
            
            extracted_data = llm.extract_yaml_data(full_content, schema)
            
            for key in ['formation', 'carriere', 'distinctions', 'controverses', 'mots_cles']:
                if key not in extracted_data or extracted_data[key] is None:
                    extracted_data[key] = []
                elif isinstance(extracted_data[key], str):
                    extracted_data[key] = [item.strip() for item in extracted_data[key].split(',') if item.strip()]
            
            extracted_data['wikipedia_url'] = page.url
            extracted_data['wikipedia_title'] = page.title
            extracted_data['factcheck_status'] = 'verified_disambiguation'
            extracted_data['verification_date'] = datetime.now().strftime('%Y-%m-%d')
            
            all_known_people = VISITED_PEOPLE.copy()
            relationships = mistral_extract_detailed_relationships(person_name, full_content, all_known_people)
            extracted_data['detailed_relationships'] = relationships
            
            institutions = extract_institutions_from_text(full_content)
            extracted_data['linked_institutions'] = institutions
            
            EXPLORATION_STATS['factcheck_disambiguation'] += 1
            
            return extracted_data
            
        except Exception as e2:
            logger.error(f"‚ùå √âchec r√©solution ambigu√Øt√© : {e2}")
            EXPLORATION_STATS['factcheck_failed'] += 1
            return None
            
    except wikipedia.exceptions.PageError:
        logger.warning(f"‚ùå Pas de page Wikipedia pour : {person_name}")
        EXPLORATION_STATS['factcheck_not_found'] += 1
        return None
        
    except Exception as e:
        logger.error(f"‚ùå Erreur factcheck {person_name} : {e}")
        EXPLORATION_STATS['factcheck_failed'] += 1
        EXPLORATION_STATS['errors'] += 1
        return None

def extract_institutions_from_text(text: str) -> List[str]:
    """
    Extrait les institutions/organisations mentionn√©es dans un texte
    """
    prompt = f"""
Extrais toutes les institutions, organisations, entreprises mentionn√©es dans ce texte.

TEXTE :
{text[:2000]}

Retourne uniquement les noms d'institutions IMPORTANTES et SIGNIFICATIVES.
Format JSON : {{"institutions": ["Institution 1", "Institution 2"]}}

Maximum 15 institutions, tri√©es par importance.
"""
    
    try:
        chat_response = llm.client.chat.complete(
            model=llm.model,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        
        if chat_response.choices and chat_response.choices[0].message:
            result = json.loads(chat_response.choices[0].message.content)
            institutions = result.get('institutions', [])
            return institutions[:15]
        
        return []
        
    except Exception as e:
        logger.error(f"Erreur extraction institutions : {e}")
        return []

def validate_person_relevance_comprehensive(person: PersonEntity, original_query: str) -> Tuple[bool, float, str]:
    """
    ‚úÖ Validation COMPL√àTE de la pertinence d'une personne
    Retourne (is_valid, confidence_score, detailed_reason)
    """
    logger.info(f"‚úÖ Validation compl√®te : {person.name} (profondeur {person.depth})")
    
    # Profondeur 0 : sujet principal, toujours valid√© avec score max
    if person.depth == 0:
        return (True, 1.0, "Sujet principal de la requ√™te")
    
    # R√©cup√©rer le contexte de la personne
    context = ""
    if person.wikipedia_data:
        bio = person.wikipedia_data.get('bio_detaillee', '')
        carriere = person.wikipedia_data.get('carriere', [])
        context = f"Bio: {bio}\nCarri√®re: {', '.join(carriere[:5])}"
    
    prompt = f"""
Tu es un expert en validation de donn√©es et fact-checking journalistique.

REQU√äTE ORIGINALE : "{original_query}"
PERSONNE √Ä VALIDER : "{person.name}"
PROFONDEUR DE RECHERCHE : {person.depth} (0 = sujet principal, 1-3 = degr√©s de s√©paration)
TROUV√âE VIA : "{person.found_via}"

CONTEXTE BIOGRAPHIQUE :
{context}

Ta mission : d√©terminer si cette personne est PERTINENTE et JUSTIFIABLE dans le contexte de la requ√™te.

CRIT√àRES DE VALIDATION STRICTS :

Profondeur 1 (1er degr√©) :
- Score ‚â• 0.8 : Lien DIRECT et DOCUMENT√â (famille proche, associ√© direct, collaborateur cl√©)
- Score 0.6-0.8 : Lien SIGNIFICATIF (relation professionnelle importante)
- Score < 0.6 : REJETER (lien trop faible ou indirect)

Profondeur 2 (2√®me degrÔøΩÔøΩ) :
- Score ‚â• 0.7 : Lien IMPORTANT via une personne cl√© (membre m√™me r√©seau, collaborateur de collaborateur)
- Score 0.6-0.7 : Lien MOD√âR√â (connexion professionnelle indirecte mais significative)
- Score < 0.6 : REJETER (trop √©loign√© de la requ√™te)

Profondeur 3 (3√®me degr√©) :
- Score ‚â• 0.65 : Lien NOTABLE (m√™me sph√®re d'influence, m√™me r√©seau √©largi)
- Score < 0.65 : REJETER (connexion trop t√©nue)

EXEMPLES CONCRETS :

Requ√™te "Jeffrey Epstein" :
- Ghislaine Maxwell (profondeur 1) ‚Üí Score 0.95 (associ√©e directe document√©e)
- Bill Clinton (profondeur 1) ‚Üí Score 0.85 (relation document√©e, voyages communs)
- Prince Andrew (profondeur 1) ‚Üí Score 0.90 (relation document√©e, accusations)
- Chelsea Clinton (profondeur 2, via Bill) ‚Üí Score 0.40 REJET√â (lien familial indirect non pertinent)
- Tony Blair (profondeur 2, via Prince Andrew) ‚Üí Score 0.60 (limite, relation indirecte)

Requ√™te "Le Si√®cle" :
- Henri de Castries (profondeur 1) ‚Üí Score 0.95 (membre confirm√©)
- Bernard Arnault (profondeur 1) ‚Üí Score 0.90 (membre du club)
- Claude B√©b√©ar (profondeur 2, via Castries) ‚Üí Score 0.75 (mentor, m√™me r√©seau)
- Emmanuel Macron (profondeur 1) ‚Üí Score 0.85 (participant document√©)

ANALYSE REQUISE :
1. Pertinence du lien par rapport √† la requ√™te originale
2. Force de la connexion (document√©e, v√©rifiable)
3. Justification journalistique (pourquoi cette personne est importante dans ce contexte)
4. Score de confiance (0.0 √† 1.0)

Retourne un JSON :
{{
  "is_relevant": true ou false,
  "confidence_score": 0.0 √† 1.0,
  "detailed_reason": "Explication d√©taill√©e et factuelle (2-3 phrases)",
  "connection_strength": "direct" ou "indirect" ou "weak",
  "journalistic_justification": "Justification √©ditoriale de l'inclusion"
}}

Sois STRICT : privil√©gie la QUALIT√â sur la QUANTIT√â. Un r√©seau de 20 personnes pertinentes vaut mieux que 100 avec des liens faibles.
"""
    
    try:
        chat_response = llm.client.chat.complete(
            model=llm.model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        
        if chat_response.choices and chat_response.choices[0].message:
            result = json.loads(chat_response.choices[0].message.content)
            
            is_relevant = result.get('is_relevant', False)
            confidence = result.get('confidence_score', 0.0)
            reason = result.get('detailed_reason', 'Pas de raison fournie')
            justification = result.get('journalistic_justification', '')
            
            # Combiner raison et justification
            full_reason = f"{reason} | Justification √©ditoriale : {justification}"
            
            EXPLORATION_STATS['validations_performed'] += 1
            
            if is_relevant and confidence >= CONFIDENCE_THRESHOLD:
                logger.info(f"‚úÖ {person.name} ‚Üí VALID√â (score: {confidence:.2f})")
                EXPLORATION_STATS['validations_passed'] += 1
                return (True, confidence, full_reason)
            else:
                logger.warning(f"‚ùå {person.name} ‚Üí REJET√â (score: {confidence:.2f}) : {reason}")
                EXPLORATION_STATS['validations_rejected'] += 1
                return (False, confidence, full_reason)
        
        return (False, 0.0, "Erreur de validation")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur validation : {e}")
        EXPLORATION_STATS['errors'] += 1
        return (False, 0.0, f"Erreur technique : {e}")

def explore_network_exponential(initial_query: str, current_depth: int = 0, 
                               max_depth: int = MAX_DEPTH, initial_query_type: str = None) -> None:
    """
    üå≥ Exploration EXPONENTIELLE du r√©seau (tous les chemins, pas de limite)
    Exploration compl√®te niveau par niveau
    """
    global VISITED_PEOPLE, VISITED_ORGS, ALL_FOUND_ENTITIES, ORIGINAL_QUERY
    
    if current_depth >= max_depth:
        logger.info(f"üõë Profondeur maximale atteinte ({max_depth})")
        return
    
    logger.info(f"\n{'='*70}")
    logger.info(f"üå≥ NIVEAU {current_depth + 1}/{max_depth} : {initial_query}")
    logger.info(f"{'='*70}")
    
    # PHASE 1 : MISTRAL IDENTIFIE LES ENTIT√âS
    # Passer le hint de type si disponible (uniquement au niveau 0)
    query_type_hint = initial_query_type if current_depth == 0 else None
    # Passer le hint de type si disponible
    query_type_hint = None
    if current_depth == 0 and hasattr(explore_network_exponential, '_initial_query_type'):
        query_type_hint = explore_network_exponential._initial_query_type
    
    entities = mistral_identify_entities_comprehensive(initial_query, query_type_hint=query_type_hint)
    
    if not entities:
        logger.warning("‚ùå Aucune entit√© identifi√©e par Mistral")
        return
    
    main_subject = entities.get('main_subject', '')
    subject_type = entities.get('subject_type', 'personne')
    people = entities.get('people', [])
    institutions = entities.get('institutions', [])
    
    # Si c'est un people_group, traiter comme une liste de personnes, pas comme institution
    if subject_type == 'people_group':
        subject_type = 'personne'  # Traiter comme des personnes
        logger.info(f"üéØ Requ√™te de type 'people_group' d√©tect√©e - focus sur les personnes")
    
   # Ajouter le sujet principal UNIQUEMENT si c'est une personne unique au niveau racine
    # Conditions: personne, nom pr√©sent, non d√©j√† dans la liste, profondeur 0, et pas un terme g√©n√©rique
    if subject_type == 'personne' and main_subject and main_subject not in people and current_depth == 0:
        if not is_generic_people_term(main_subject):
            people.insert(0, main_subject)
    
    # Tracker les institutions
    for inst in institutions:
        if inst not in VISITED_ORGS:
            VISITED_ORGS.add(inst)
            institution_entity = InstitutionEntity(
                name=inst,
                depth=current_depth,
                found_via=initial_query if current_depth > 0 else 'requ√™te principale'
            )
            ALL_FOUND_ENTITIES.append(institution_entity)
            logger.info(f"üè¢ Institution ajout√©e : {inst}")
    
    # PHASE 2 : FACTCHECK WIKIPEDIA POUR CHAQUE PERSONNE
    people_to_explore_next = []
    
    for person_name in people:
        if person_name in VISITED_PEOPLE:
            logger.info(f"‚è≠Ô∏è  {person_name} d√©j√† trait√©, skip")
            continue
        
        VISITED_PEOPLE.add(person_name)
        
        logger.info(f"\n{'‚îÄ'*60}")
        logger.info(f"üîç Traitement : {person_name} (profondeur {current_depth})")
        
        # Cr√©er l'entit√© personne
        person_entity = PersonEntity(
            name=person_name,
            depth=current_depth,
            found_via=initial_query if current_depth > 0 else 'requ√™te principale',
            query=ORIGINAL_QUERY
        )
        
        # Factcheck Wikipedia
        wiki_data = wikipedia_factcheck_person_rigorous(person_name)
        
        if not wiki_data:
            logger.warning(f"‚ùå {person_name} non v√©rifi√© sur Wikipedia, ignor√©")
            person_entity.factcheck_status = "failed"
            continue
        
        person_entity.wikipedia_data = wiki_data
        person_entity.factcheck_status = wiki_data.get('factcheck_status', 'verified')
        
        # Stocker les relations et institutions
        relationships = wiki_data.get('detailed_relationships', [])
        person_entity.relationships = relationships
        person_entity.organizations = wiki_data.get('linked_institutions', [])
        
        # Ajouter √† la liste des entit√©s
        ALL_FOUND_ENTITIES.append(person_entity)
        
        logger.info(f"‚úÖ {person_name} fackcheck√© (profondeur {current_depth})")
        logger.info(f"   - {len(relationships)} relations d√©taill√©es")
        logger.info(f"   - {len(person_entity.organizations)} institutions li√©es")
        
        # Collecter les personnes √† explorer au niveau suivant
        if current_depth < max_depth - 1 and EXPONENTIAL_EXPLORATION:
            for rel in relationships:
                if rel.person_to not in VISITED_PEOPLE and rel.confidence >= 0.7:
                    people_to_explore_next.append(rel.person_to)
        
        # Petit d√©lai pour √©viter de surcharger Wikipedia
        time.sleep(0.5)
    
    # PHASE 3 : EXPLORATION R√âCURSIVE DU NIVEAU SUIVANT
    if current_depth < max_depth - 1 and people_to_explore_next:
        logger.info(f"\nüîÑ Exploration du niveau suivant : {len(people_to_explore_next)} personnes")
        
        # Explorer TOUTES les personnes du niveau suivant (exponentiel)
        for next_person in people_to_explore_next:
            if next_person not in VISITED_PEOPLE:
                explore_network_exponential(
                    next_person,
                    current_depth + 1,
                    max_depth
                )

def final_validation_before_commit(entities: List[PersonEntity], original_query: str) -> Tuple[List[PersonEntity], List[PersonEntity]]:
    """
    üéØ VALIDATION FINALE de toutes les personnes AVANT commit
    Filtre rigoureux pour garantir la qualit√© journalistique
    """
    logger.info(f"\n{'='*70}")
    logger.info(f"üéØ VALIDATION FINALE AVANT COMMIT")
    logger.info(f"{'='*70}")
    logger.info(f"üìä {len(entities)} personnes √† valider contre la requ√™te : '{original_query}'")
    
    validated_entities = []
    rejected_entities = []
    
    for person_entity in entities:
        if not isinstance(person_entity, PersonEntity):
            continue
        
        # Validation compl√®te
        is_valid, confidence, reason = validate_person_relevance_comprehensive(
            person_entity,
            original_query
        )
        
        person_entity.is_validated = is_valid
        person_entity.validation_score = confidence
        person_entity.validation_reason = reason
        
        VALIDATION_SCORES[person_entity.name] = {
            'score': confidence,
            'validated': is_valid,
            'reason': reason
        }
        
        if is_valid:
            validated_entities.append(person_entity)
        else:
            rejected_entities.append(person_entity)
    
    logger.info(f"\n‚úÖ Validation finale : {len(validated_entities)} accept√©es, {len(rejected_entities)} rejet√©es")
    
    return validated_entities, rejected_entities

def create_person_file_comprehensive(person: PersonEntity, all_institutions: List[str]) -> bool:
    """
    üìù Cr√©ation de fiche personne COMPL√àTE avec relations d√©taill√©es pour Obsidian
    """
    person_name = person.name
    person_data = person.wikipedia_data
    depth = person.depth
    found_via = person.found_via
    validation_score = person.validation_score
    
    if not person_data:
        logger.error(f"‚ùå Pas de donn√©es Wikipedia pour {person_name}")
        return False
    
    personnes_folder = Path("personnes")
    personnes_folder.mkdir(exist_ok=True)
    
    safe_filename = re.sub(r'[^\w\s-]', '', person_name).strip().replace(' ', '-')
    file_path = personnes_folder / f"{safe_filename}.md"
    
    if file_path.exists():
        logger.info(f"‚ÑπÔ∏è  {person_name} existe d√©j√†, ignor√©")
        return False
    
    # ========== CONSTRUCTION DU CONTENU MARKDOWN ==========
    
    # En-t√™te avec contexte de d√©couverte
    discovery_header = ""
    if depth == 0:
        discovery_header = f"> üéØ **Sujet principal de la recherche**\n> Score de pertinence : {validation_score:.0%}\n"
    else:
        discovery_header = f"> üîç **D√©couvert via [[{found_via}]]** (niveau {depth})\n> Score de pertinence : {validation_score:.0%}\n"
    
    # Biographie
    bio_courte = person_data.get('bio_courte', '')
    bio_detaillee = person_data.get('bio_detaillee', '')
    
    bio_section = f"""## Biographie

{bio_detaillee if bio_detaillee else bio_courte}
"""
    
    # Section Informations personnelles
    info_section = "\n## Informations personnelles\n\n"
    
    if person_data.get('date_naissance'):
        info_section += f"**Date de naissance** : {person_data['date_naissance']}\n"
    if person_data.get('date_deces'):
        info_section += f"**Date de d√©c√®s** : {person_data['date_deces']}\n"
    if person_data.get('lieu_naissance'):
        info_section += f"**Lieu de naissance** : {person_data['lieu_naissance']}\n"
    if person_data.get('nationalite'):
        info_section += f"**Nationalit√©** : {person_data['nationalite']}\n"
    if person_data.get('statut_actuel'):
        info_section += f"**Statut** : {person_data['statut_actuel']}\n"
    
    # Section Formation
    formation_section = ""
    formation = person_data.get('formation', [])
    if formation and len(formation) > 0:
        formation_section = "\n## Formation\n\n"
        for item in formation[:10]:
            formation_section += f"- {item}\n"
    
    # Section Carri√®re
    carriere_section = ""
    carriere = person_data.get('carriere', [])
    if carriere and len(carriere) > 0:
        carriere_section = "\n## Carri√®re\n\n"
        for item in carriere[:15]:
            carriere_section += f"- {item}\n"
    
    # Section Organisations et Institutions (avec liens Obsidian)
    org_section = ""
    institutions = person_data.get('linked_institutions', [])
    all_orgs = list(set(institutions + all_institutions))
    
    if all_orgs:
        org_section = "\n## Organisations et Institutions\n\n"
        for org in all_orgs[:20]:
            org_section += f"- [[{org}]]\n"
    
    # Section RELATIONS D√âTAILL√âES (c≈ìur de l'Obsidian graph)
    relations_section = ""
    relationships = person.relationships
    
    if relationships and len(relationships) > 0:
        relations_section = "\n## R√©seau et Connexions\n\n"
        relations_section += f"*{len(relationships)} relations document√©es*\n\n"
        
        # Grouper par type de relation
        relations_by_type = defaultdict(list)
        for rel in relationships:
            relations_by_type[rel.relationship_type].append(rel)
        
        for rel_type, rels in relations_by_type.items():
            relations_section += f"\n### {rel_type.capitalize()}\n\n"
            for rel in sorted(rels, key=lambda x: x.confidence, reverse=True)[:10]:
                # Format Obsidian avec description d√©taill√©e
                relations_section += f"- [[{rel.person_to}]] : {rel.description} *(confiance: {rel.confidence:.0%})*\n"
    
    # Section Distinctions
    distinctions_section = ""
    distinctions = person_data.get('distinctions', [])
    if distinctions and len(distinctions) > 0:
        distinctions_section = "\n## Distinctions et Prix\n\n"
        for item in distinctions[:10]:
            distinctions_section += f"- {item}\n"
    
    # Section Controverses (transparence journalistique)
    controverses_section = ""
    controverses = person_data.get('controverses', [])
    if controverses and len(controverses) > 0:
        controverses_section = "\n## Controverses\n\n"
        for item in controverses[:10]:
            controverses_section += f"- {item}\n"
    
    # Mots-cl√©s (tags Obsidian)
    mots_cles = person_data.get('mots_cles', [])
    tags_line = ""
    if mots_cles:
        tags_line = "\n**Tags** : " + " ¬∑ ".join([f"#{tag.replace(' ', '-')}" for tag in mots_cles[:10]]) + "\n"
    
    # Footer avec m√©tadonn√©es de v√©rification
    footer = f"""
---

## M√©tadonn√©es et V√©rification

**Source principale** : [Wikipedia]({person_data.get('wikipedia_url', '')})  
**Titre Wikipedia** : {person_data.get('wikipedia_title', person_name)}  
**Statut de v√©rification** : ‚úÖ {person_data.get('factcheck_status', 'verified')}  
**Date de v√©rification** : {person_data.get('verification_date', datetime.now().strftime('%Y-%m-%d'))}  
**Longueur article Wikipedia** : {person_data.get('content_length', 0)} caract√®res  
**Niveau de notori√©t√©** : {person_data.get('niveau_notoriete', 'N/A')}/10  
**Score de pertinence** : {validation_score:.0%}  
**Profondeur de recherche** : {depth}  
**Requ√™te originale** : "{person.original_query}"  

{tags_line}

*Fiche cr√©√©e le {datetime.now().strftime('%Y-%m-%d √† %H:%M')} via l'≈íil de Dieu (exploration r√©cursive niveau {depth})*
"""
    
    # ========== ASSEMBLAGE FINAL ==========
    content = f"""{discovery_header}
{bio_section}
{info_section}
{formation_section}
{carriere_section}
{org_section}
{relations_section}
{distinctions_section}
{controverses_section}
{footer}
"""
    
    # ========== M√âTADONN√âES FRONTMATTER ==========
    metadata = {
        'type': 'personne',
        'nom_complet': person_data.get('nom_complet_verifie', person_name),
        'prenoms': person_name.split()[0] if ' ' in person_name else person_name,
        'date_naissance': person_data.get('date_naissance', ''),
        'date_deces': person_data.get('date_deces', ''),
        'lieu_naissance': person_data.get('lieu_naissance', ''),
        'nationalite': person_data.get('nationalite', ''),
        'genre': person_data.get('genre', ''),
        'statut': person_data.get('statut_actuel', ''),
        'bio': bio_courte,
        'formation': formation[:10],
        'carriere': carriere[:15],
        'affiliations': all_orgs[:20],
        'distinctions': distinctions[:10],
        'controverses': controverses[:10],
        'liens': [rel.person_to for rel in relationships[:20]],
        'relations_detaillees': [rel.to_dict() for rel in relationships[:20]],
        'presse': [],
        'sources': [person_data.get('wikipedia_url', '')],
        'statut_note': 'verifie_wikipedia',
        'tags': ['elite', 'wikipedia', f'niveau-{depth}', 'oeil-de-dieu'] + mots_cles[:5],
        'date_creation_note': datetime.now().strftime('%Y-%m-%d'),
        'found_via': found_via,
        'search_depth': depth,
        'verification_status': person_data.get('factcheck_status', 'verified'),
        'verification_date': person_data.get('verification_date', ''),
        'validation_score': round(validation_score, 2),
        'validation_reason': person.validation_reason,
        'original_query': person.original_query,
        'niveau_notoriete': person_data.get('niveau_notoriete', ''),
        'relationships_count': len(relationships),
        'institutions_count': len(all_orgs),
        'wikipedia_content_length': person_data.get('content_length', 0)
    }
    
    # ========== √âCRITURE DU FICHIER ==========
    post = frontmatter.Post(content, **metadata)
    
    try:
        with open(file_path, 'wb') as f:
            frontmatter.dump(post, f)
        
        person.created_file_path = str(file_path)
        CREATED_FILES.append(str(file_path))
        
        logger.info(f"‚úÖ Fiche cr√©√©e : {file_path}")
        logger.info(f"   - {len(relationships)} relations d√©taill√©es")
        logger.info(f"   - {len(all_orgs)} institutions")
        logger.info(f"   - Score de validation : {validation_score:.0%}")
        
        EXPLORATION_STATS['files_created'] += 1
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation fiche {person_name} : {e}")
        EXPLORATION_STATS['errors'] += 1
        return False

def create_institution_file_comprehensive(institution: InstitutionEntity) -> bool:
    """
    üìù Cr√©ation de fiche institution COMPL√àTE
    """
    institution_name = institution.name
    depth = institution.depth
    found_via = institution.found_via
    
    institutions_folder = Path("institutions")
    institutions_folder.mkdir(exist_ok=True)
    
    safe_filename = re.sub(r'[^\w\s-]', '', institution_name).strip().replace(' ', '-')
    file_path = institutions_folder / f"{safe_filename}.md"
    
    if file_path.exists():
        logger.info(f"‚ÑπÔ∏è  Institution {institution_name} existe d√©j√†, ignor√©")
        return False
    
    # Essayer de trouver sur Wikipedia
    try:
        page = wikipedia.page(institution_name, auto_suggest=True)
        summary = page.summary[:800]
        wiki_url = page.url
        verified = True
        
        # Extraire plus d'infos
        full_content = page.content[:3000]
        
        schema = """
        {
          "description_detaillee": "Description d√©taill√©e de l'institution",
          "date_fondation": "Date de fondation",
          "fondateurs": "Noms des fondateurs",
          "siege_social": "Localisation du si√®ge",
          "type_organisation": "Type d'organisation (entreprise, club, think tank, etc.)",
          "domaine_activite": "Domaine d'activit√© principal",
          "membres_notables": "Membres ou dirigeants notables (liste)",
          "influence": "Description de l'influence et du r√¥le"
        }
        """
        
        extracted_data = llm.extract_yaml_data(full_content, schema)
        
        description = extracted_data.get('description_detaillee', summary)
        membres = extracted_data.get('membres_notables', [])
        
        if isinstance(membres, str):
            membres = [m.strip() for m in membres.split(',') if m.strip()]
        
        institution.members = membres
        
    except:
        summary = f"Institution identifi√©e dans le r√©seau de pouvoir li√© √† : {found_via}"
        wiki_url = ""
        verified = False
        description = summary
        extracted_data = {}
    
    # D√©couverte
    discovery_text = ""
    if depth > 0:
        discovery_text = f"> üîç **D√©couvert via [[{found_via}]]** (niveau {depth})\n"
    else:
        discovery_text = f"> üéØ **Sujet principal de la recherche**\n"
    
    # Membres (liens Obsidian)
    membres_section = ""
    if institution.members:
        membres_section = f"\n## Membres et Dirigeants\n\n"
        for membre in institution.members[:20]:
            membres_section += f"- [[{membre}]]\n"
    
    content = f"""{discovery_text}

## Description

{description}

{membres_section}

---

## M√©tadonn√©es

**Type** : Institution / Organisation  
**Cat√©gorie** : {extracted_data.get('type_organisation', 'N/A')}  
**Fondation** : {extracted_data.get('date_fondation', 'N/A')}  
**Si√®ge** : {extracted_data.get('siege_social', 'N/A')}  
**Domaine** : {extracted_data.get('domaine_activite', 'N/A')}  
**Source** : {'[Wikipedia](' + wiki_url + ')' if wiki_url else 'R√©seau Mistral'}  
**Statut de v√©rification** : {'‚úÖ Wikipedia' if verified else '‚ö†Ô∏è √Ä v√©rifier'}  
**Date d'ajout** : {datetime.now().strftime('%Y-%m-%d')}  

*Fiche cr√©√©e via l'≈íil de Dieu (niveau {depth})*
"""
    
    metadata = {
        'type': 'institution',
        'nom': institution_name,
        'description': description,
        'type_organisation': extracted_data.get('type_organisation', ''),
        'date_fondation': extracted_data.get('date_fondation', ''),
        'siege': extracted_data.get('siege_social', ''),
        'domaine': extracted_data.get('domaine_activite', ''),
        'membres': institution.members[:20],
        'sources': [wiki_url] if wiki_url else [],
        'statut_note': 'verifie_wikipedia' if verified else 'a_verifier',
        'tags': ['institution', 'elite', f'niveau-{depth}', 'oeil-de-dieu'],
        'date_creation_note': datetime.now().strftime('%Y-%m-%d'),
        'found_via': found_via,
        'search_depth': depth,
        'verified': verified
    }
    
    post = frontmatter.Post(content, **metadata)
    
    try:
        with open(file_path, 'wb') as f:
            frontmatter.dump(post, f)
        
        institution.created_file_path = str(file_path)
        CREATED_FILES.append(str(file_path))
        
        logger.info(f"‚úÖ Institution cr√©√©e : {file_path}")
        EXPLORATION_STATS['institutions_created'] += 1
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation institution {institution_name} : {e}")
        EXPLORATION_STATS['errors'] += 1
        return False

def generate_exploration_report(query: str, validated: List[PersonEntity], 
                               rejected: List[PersonEntity]) -> str:
    """
    üìä G√©n√®re un rapport d√©taill√© de l'exploration
    """
    report = f"""
{'='*70}
üìä RAPPORT D'EXPLORATION - ≈íIL DE DIEU
{'='*70}

REQU√äTE ORIGINALE : "{query}"
DATE : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{'='*70}
STATISTIQUES GLOBALES
{'='*70}

Profondeur d'exploration : {MAX_DEPTH} niveaux
Mode : {'EXPONENTIEL (complet)' if EXPONENTIAL_EXPLORATION else 'LIMIT√â'}
Seuil de confiance : {CONFIDENCE_THRESHOLD:.0%}

Appels Mistral : {EXPLORATION_STATS['mistral_calls']}
Entit√©s identifi√©es (Mistral) : {EXPLORATION_STATS['entities_identified']}
Institutions identifi√©es : {EXPLORATION_STATS['institutions_identified']}
Relations extraites : {EXPLORATION_STATS['relationships_extracted']}

Factchecks Wikipedia :
  - R√©ussis : {EXPLORATION_STATS['factcheck_success']}
  - Non trouv√©s : {EXPLORATION_STATS['factcheck_not_found']}
  - Ambigu√Øt√©s r√©solues : {EXPLORATION_STATS['factcheck_disambiguation']}
  - √âchecs : {EXPLORATION_STATS['factcheck_failed']}

Validations :
  - Effectu√©es : {EXPLORATION_STATS['validations_performed']}
  - Accept√©es : {EXPLORATION_STATS['validations_passed']}
  - Rejet√©es : {EXPLORATION_STATS['validations_rejected']}

Fichiers cr√©√©s :
  - Personnes : {EXPLORATION_STATS['files_created']}
  - Institutions : {EXPLORATION_STATS['institutions_created']}
  - Total : {EXPLORATION_STATS['files_created'] + EXPLORATION_STATS['institutions_created']}

Erreurs : {EXPLORATION_STATS['errors']}

{'='*70}
PERSONNES VALID√âES ({len(validated)})
{'='*70}

"""
    
    # Trier par score de validation
    validated_sorted = sorted(validated, key=lambda x: x.validation_score, reverse=True)
    
    for i, person in enumerate(validated_sorted, 1):
        report += f"""
{i}. {person.name}
   Profondeur : {person.depth}
   Score : {person.validation_score:.0%}
   Trouv√© via : {person.found_via}
   Relations : {len(person.relationships)}
   Raison : {person.validation_reason[:100]}...
"""
    
    report += f"""
{'='*70}
PERSONNES REJET√âES ({len(rejected)})
{'='*70}

"""
    
    rejected_sorted = sorted(rejected, key=lambda x: x.validation_score, reverse=True)
    
    for i, person in enumerate(rejected_sorted, 1):
        report += f"""
{i}. {person.name}
   Profondeur : {person.depth}
   Score : {person.validation_score:.0%}
   Raison du rejet : {person.validation_reason[:100]}...
"""
    
    report += f"""
{'='*70}
ANALYSE DE QUALIT√â
{'='*70}

Taux de validation : {len(validated)/(len(validated)+len(rejected))*100:.1f}%
Score moyen des valid√©s : {sum(p.validation_score for p in validated)/len(validated):.0%}
Score moyen des rejet√©s : {sum(p.validation_score for p in rejected)/len(rejected) if rejected else 0:.0%}

Distribution par profondeur :
"""
    
    for depth in range(MAX_DEPTH):
        count = len([p for p in validated if p.depth == depth])
        report += f"  Niveau {depth} : {count} personnes\n"
    
    report += f"""
{'='*70}
FIN DU RAPPORT
{'='*70}
"""
    
    return report

def is_generic_people_term(name: str) -> bool:
    """
    V√©rifie si un nom est un terme g√©n√©rique (pas une personne sp√©cifique)
    """
    generic_terms = [
        'dirigeants', 'membres', 'pr√©sidents', 'ministres', 'executives',
        'leaders', 'cadres', 'responsables', 'directeurs', 'personnes',
        'gens', 'individus', 'acteurs', 'participants', 'repr√©sentants'
    ]
    
    name_lower = name.lower().strip()
    
    # V√©rifier correspondance exacte
    if name_lower in generic_terms:
        return True
    
    # V√©rifier correspondance par mots complets (pattern compil√© une seule fois)
    if not hasattr(is_generic_people_term, '_pattern'):
        # Cr√©er un pattern combin√© pour tous les termes
        escaped_terms = [re.escape(term) for term in generic_terms]
        pattern = r'\b(?:' + '|'.join(escaped_terms) + r')\b'
        is_generic_people_term._pattern = re.compile(pattern)
    
    return bool(is_generic_people_term._pattern.search(name_lower))
    name_lower = name.lower()
    return any(term in name_lower for term in generic_terms)

def main(query: str = None):
    """
    üß† ≈íIL DE DIEU - Exploration exponentielle avec validation finale
    Niveau journalistique : rigueur, tra√ßabilit√©, v√©rification
    """
    global VISITED_PEOPLE, VISITED_ORGS, ALL_FOUND_ENTITIES, ORIGINAL_QUERY
    global EXPLORATION_STATS, RELATIONSHIPS_GRAPH, VALIDATION_SCORES, CREATED_FILES
    
    # R√©initialisation compl√®te
    VISITED_PEOPLE = set()
    VISITED_ORGS = set()
    ALL_FOUND_ENTITIES = []
    EXPLORATION_STATS = defaultdict(int)
    RELATIONSHIPS_GRAPH = defaultdict(list)
    VALIDATION_SCORES = {}
    CREATED_FILES = []
    
    print("\n" + "="*70)
    print("üß† ≈íIL DE DIEU - Construction de r√©seau de pouvoir")
    print("="*70)
    print("\nüìã Mode d'op√©ration :")
    print("  1. Mistral identifie les entit√©s (connaissance g√©n√©rale)")
    print("  2. Wikipedia factcheck et enrichit (sources v√©rifiables)")
    print("  3. Exploration EXPONENTIELLE sur 3 niveaux (tous les chemins)")
    print("  4. Extraction de relations D√âTAILL√âES avec descriptions")
    print("  5. Validation FINALE de toutes les personnes avant commit")
    print("  6. Cr√©ation de fiches Obsidian avec liens [[personne]]")
    print(f"\n‚öôÔ∏è  Param√®tres :")
    print(f"  - Profondeur maximale : {MAX_DEPTH}")
    print(f"  - Seuil de confiance : {CONFIDENCE_THRESHOLD:.0%}")
    print(f"  - Mode exponentiel : {'OUI' if EXPONENTIAL_EXPLORATION else 'NON'}")
    print("="*70)
    
    if not query:
        print("\nExemples de requ√™tes :")
        print("  - Le Si√®cle")
        print("  - Jeffrey Epstein")
        print("  - Emmanuel Macron")
        print("  - Groupe Bilderberg")
        print("  - Bernard Arnault")
        print("="*70)
        
        query = input("\nüéØ Entit√© √† explorer : ").strip()
    
    if not query:
        logger.error("‚ùå Requ√™te vide, abandon")
        return
    
    ORIGINAL_QUERY = query
    
    logger.info(f"üöÄ Lancement de l'exploration : '{query}'")
    start_time = time.time()
    
    # ========== PHASE 0 : R√âPONSE DIRECTE √Ä LA REQU√äTE ==========
    print(f"\nüéØ Phase 0 : Analyse et r√©ponse directe √† la requ√™te...\n")
    
    initial_answer = answer_initial_query_directly(query)
    
    if not initial_answer:
        logger.warning("‚ùå Impossible de r√©pondre √† la requ√™te")
        return
    
    query_type = initial_answer.get('query_type', 'unknown')
    interpretation = initial_answer.get('interpretation', '')
    initial_people = initial_answer.get('people', [])
    initial_institutions = initial_answer.get('institutions', [])
    
    print(f"\n‚úÖ Analyse de la requ√™te :")
    print(f"   - Type : {query_type}")
    print(f"   - Interpr√©tation : {interpretation}")
    print(f"   - {len(initial_people)} personnes identifi√©es initialement")
    print(f"   - {len(initial_institutions)} institutions identifi√©es")
    
    # Afficher la r√©ponse directe
    if query_type == 'people_group':
        print(f"\nüìã R√âPONSE DIRECTE - Liste des personnes :")
        for i, person in enumerate(initial_people, 1):
            print(f"   {i}. {person}")
    elif query_type == 'single_person':
        print(f"\nüë§ R√âPONSE DIRECTE - Personne principale : {initial_answer.get('main_subject', '')}")
        print(f"   R√©seau imm√©diat ({len(initial_people)-1} personnes) :")
        for person in initial_people[1:]:
            print(f"   - {person}")
    elif query_type == 'institution':
        print(f"\nüè¢ R√âPONSE DIRECTE - Institution : {initial_answer.get('main_subject', '')}")
        print(f"   Membres/Dirigeants ({len(initial_people)} personnes) :")
        for i, person in enumerate(initial_people, 1):
            print(f"   {i}. {person}")
    
    # ========== PHASE 1 : EXPLORATION EXPONENTIELLE ==========
    print(f"\nüå≥ Phase 1 : Exploration exponentielle (3 niveaux)...\n")
    explore_network_exponential(query, current_depth=0, max_depth=MAX_DEPTH, initial_query_type=query_type)
    # Stocker le type de requ√™te pour l'exploration
    explore_network_exponential._initial_query_type = query_type
    
    try:
        # ========== PHASE 1 : EXPLORATION EXPONENTIELLE ==========
        print(f"\nüå≥ Phase 1 : Exploration exponentielle (3 niveaux)...\n")
        explore_network_exponential(query, current_depth=0, max_depth=MAX_DEPTH)
    finally:
        # Nettoyer l'attribut temporaire apr√®s utilisation (m√™me en cas d'exception)
        if hasattr(explore_network_exponential, '_initial_query_type'):
            delattr(explore_network_exponential, '_initial_query_type')
    
    if not ALL_FOUND_ENTITIES:
        logger.warning("‚ùå Aucune entit√© trouv√©e")
        return
    
    # S√©parer personnes et institutions
    people_entities = [e for e in ALL_FOUND_ENTITIES if isinstance(e, PersonEntity)]
    institution_entities = [e for e in ALL_FOUND_ENTITIES if isinstance(e, InstitutionEntity)]
    
    print(f"\n‚úÖ Exploration termin√©e :")
    print(f"   - {len(people_entities)} personnes d√©couvertes")
    print(f"   - {len(institution_entities)} institutions d√©couvertes")
    print(f"   - {EXPLORATION_STATS['relationships_extracted']} relations extraites")

        # ========== PHASE 2 : VALIDATION FINALE AVANT COMMIT ==========
    print(f"\nüéØ Phase 2 : Validation finale de toutes les entit√©s...\n")
    
    validated_people, rejected_people = final_validation_before_commit(
        people_entities,
        ORIGINAL_QUERY
    )
    
    print(f"\n‚úÖ Validation termin√©e :")
    print(f"   - {len(validated_people)} personnes VALID√âES")
    print(f"   - {len(rejected_people)} personnes REJET√âES")
    print(f"   - Taux de validation : {len(validated_people)/(len(validated_people)+len(rejected_people))*100:.1f}%")
    
    if not validated_people and not institution_entities:
        logger.warning("‚ùå Aucune entit√© valid√©e √† cr√©er")
        return
    
    # ========== V√âRIFICATION DES FICHIERS EXISTANTS ==========
    print(f"\nüìÇ Phase 3 : V√©rification des fichiers existants...\n")
    
    personnes_folder = Path("personnes")
    institutions_folder = Path("institutions")
    
    existing_people_files = set()
    existing_institution_files = set()
    
    if personnes_folder.exists():
        existing_people_files = {f.stem for f in personnes_folder.glob("*.md")}
        logger.info(f"üìÅ {len(existing_people_files)} fichiers personnes existants trouv√©s")
    
    if institutions_folder.exists():
        existing_institution_files = {f.stem for f in institutions_folder.glob("*.md")}
        logger.info(f"üìÅ {len(existing_institution_files)} fichiers institutions existants trouv√©s")
    
    # Filtrer les entit√©s d√©j√† existantes
    people_to_create = []
    people_already_exist = []
    
    for person in validated_people:
        safe_filename = re.sub(r'[^\w\s-]', '', person.name).strip().replace(' ', '-')
        if safe_filename in existing_people_files:
            people_already_exist.append(person.name)
            logger.info(f"‚è≠Ô∏è  {person.name} existe d√©j√†, skip")
        else:
            people_to_create.append(person)
    
    institutions_to_create = []
    institutions_already_exist = []
    
    for inst in institution_entities:
        safe_filename = re.sub(r'[^\w\s-]', '', inst.name).strip().replace(' ', '-')
        if safe_filename in existing_institution_files:
            institutions_already_exist.append(inst.name)
            logger.info(f"‚è≠Ô∏è  Institution {inst.name} existe d√©j√†, skip")
        else:
            institutions_to_create.append(inst)
    
    print(f"\nüìä Bilan des fichiers √† cr√©er :")
    print(f"   - Personnes : {len(people_to_create)} nouvelles ({len(people_already_exist)} existent d√©j√†)")
    print(f"   - Institutions : {len(institutions_to_create)} nouvelles ({len(institutions_already_exist)} existent d√©j√†)")
    
    if not people_to_create and not institutions_to_create:
        print(f"\n‚ö†Ô∏è  Toutes les entit√©s existent d√©j√†, aucune cr√©ation n√©cessaire")
        logger.info("‚úÖ Toutes les entit√©s existent d√©j√†")
        return
    
    # ========== PHASE 4 : CR√âATION DES FICHIERS ==========
    print(f"\nüìù Phase 4 : Cr√©ation des fiches...\n")
    
    all_institutions_names = [inst.name for inst in institution_entities]
    
    people_created = 0
    people_errors = 0
    
    for person in people_to_create:
        try:
            if create_person_file_comprehensive(person, all_institutions_names):
                people_created += 1
                print(f"   ‚úÖ {person.name} (score: {person.validation_score:.0%}, niveau: {person.depth})")
            else:
                people_errors += 1
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation {person.name} : {e}")
            people_errors += 1
            EXPLORATION_STATS['errors'] += 1
    
    institutions_created = 0
    institutions_errors = 0
    
    for inst in institutions_to_create:
        try:
            if create_institution_file_comprehensive(inst):
                institutions_created += 1
                print(f"   üè¢ {inst.name} (niveau: {inst.depth})")
            else:
                institutions_errors += 1
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation institution {inst.name} : {e}")
            institutions_errors += 1
            EXPLORATION_STATS['errors'] += 1
    
    # ========== PHASE 5 : G√âN√âRATION DU RAPPORT ==========
    print(f"\nüìä Phase 5 : G√©n√©ration du rapport...\n")
    
    report = generate_exploration_report(query, validated_people, rejected_people)
    
    # Sauvegarder le rapport
    reports_folder = Path("rapports")
    reports_folder.mkdir(exist_ok=True)
    
    report_filename = f"rapport_exploration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    report_path = reports_folder / report_filename
    
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"üìÑ Rapport sauvegard√© : {report_path}")
        print(f"   üìÑ Rapport sauvegard√© : {report_path}")
    except Exception as e:
        logger.error(f"‚ùå Erreur sauvegarde rapport : {e}")
    
    # ========== PHASE 6 : R√âSUM√â FINAL ==========
    elapsed_time = time.time() - start_time
    
    print("\n" + "="*70)
    print("üéâ R√âSULTAT FINAL")
    print("="*70)
    
    print(f"\nüìä STATISTIQUES COMPL√àTES :")
    print(f"   Dur√©e d'exploration : {elapsed_time:.1f} secondes ({elapsed_time/60:.1f} minutes)")
    print(f"\n   üîç D√©couverte :")
    print(f"      - Personnes d√©couvertes : {len(people_entities)}")
    print(f"      - Institutions d√©couvertes : {len(institution_entities)}")
    print(f"      - Relations extraites : {EXPLORATION_STATS['relationships_extracted']}")
    
    print(f"\n   ‚úÖ Validation :")
    print(f"      - Personnes valid√©es : {len(validated_people)}")
    print(f"      - Personnes rejet√©es : {len(rejected_people)}")
    print(f"      - Taux de validation : {len(validated_people)/(len(validated_people)+len(rejected_people))*100:.1f}%")
    
    print(f"\n   üìù Cr√©ation :")
    print(f"      - Personnes cr√©√©es : {people_created}")
    print(f"      - Personnes d√©j√† existantes : {len(people_already_exist)}")
    print(f"      - Institutions cr√©√©es : {institutions_created}")
    print(f"      - Institutions d√©j√† existantes : {len(institutions_already_exist)}")
    print(f"      - Erreurs : {people_errors + institutions_errors}")
    
    print(f"\n   üå≥ R√©partition par profondeur :")
    for depth in range(MAX_DEPTH):
        count_validated = len([p for p in validated_people if p.depth == depth])
        count_created = len([p for p in people_to_create if p.depth == depth and p.name not in people_already_exist])
        print(f"      Niveau {depth} : {count_validated} valid√©es, {count_created} cr√©√©es")
    
    print(f"\n   üîó Qualit√© du r√©seau :")
    if validated_people:
        avg_score = sum(p.validation_score for p in validated_people) / len(validated_people)
        avg_relations = sum(len(p.relationships) for p in validated_people) / len(validated_people)
        print(f"      - Score moyen de pertinence : {avg_score:.0%}")
        print(f"      - Relations moyennes par personne : {avg_relations:.1f}")
    
    print(f"\n   üìñ Factchecks Wikipedia :")
    print(f"      - R√©ussis : {EXPLORATION_STATS['factcheck_success']}")
    print(f"      - Non trouv√©s : {EXPLORATION_STATS['factcheck_not_found']}")
    print(f"      - Ambigu√Øt√©s r√©solues : {EXPLORATION_STATS['factcheck_disambiguation']}")
    print(f"      - √âchecs : {EXPLORATION_STATS['factcheck_failed']}")
    
    print(f"\n   ü§ñ Appels Mistral :")
    print(f"      - Total : {EXPLORATION_STATS['mistral_calls']}")
    
    total_created = people_created + institutions_created
    
    # ========== PHASE 7 : AFFICHAGE DES ENTIT√âS CR√â√âES ==========
    if people_created > 0:
        print(f"\nüë• PERSONNES CR√â√âES ({people_created}) :")
        for person in people_to_create:
            if person.created_file_path:
                print(f"   ‚úÖ {person.name} (score: {person.validation_score:.0%}, niveau: {person.depth})")
    
    if institutions_created > 0:
        print(f"\nüè¢ INSTITUTIONS CR√â√âES ({institutions_created}) :")
        for inst in institutions_to_create:
            if inst.created_file_path:
                print(f"   ‚úÖ {inst.name} (niveau: {inst.depth})")
    
    if people_already_exist:
        print(f"\n‚è≠Ô∏è  PERSONNES D√âJ√Ä EXISTANTES ({len(people_already_exist)}) :")
        for name in people_already_exist[:10]:
            print(f"   - {name}")
        if len(people_already_exist) > 10:
            print(f"   ... et {len(people_already_exist) - 10} autres")
    
    if institutions_already_exist:
        print(f"\n‚è≠Ô∏è  INSTITUTIONS D√âJ√Ä EXISTANTES ({len(institutions_already_exist)}) :")
        for name in institutions_already_exist[:10]:
            print(f"   - {name}")
        if len(institutions_already_exist) > 10:
            print(f"   ... et {len(institutions_already_exist) - 10} autres")
    
    # ========== PHASE 8 : COMMIT GIT ==========
    if total_created > 0:
        print("\n" + "="*70)
        print("üíæ Phase 7 : Commit Git...")
        print("="*70)
        
        commit_msg = f"""feat: üß† ≈íil de Dieu - Exploration '{query}'

Statistiques :
- {people_created} personnes cr√©√©es
- {institutions_created} institutions cr√©√©es
- {len(validated_people)} personnes valid√©es (taux: {len(validated_people)/(len(validated_people)+len(rejected_people))*100:.1f}%)
- {EXPLORATION_STATS['relationships_extracted']} relations extraites
- Exploration sur {MAX_DEPTH} niveaux
- Dur√©e : {elapsed_time:.1f}s

Qualit√© :
- Score moyen : {sum(p.validation_score for p in validated_people)/len(validated_people):.0%}
- Relations moyennes : {sum(len(p.relationships) for p in validated_people)/len(validated_people):.1f}

Factchecks Wikipedia :
- R√©ussis : {EXPLORATION_STATS['factcheck_success']}
- √âchecs : {EXPLORATION_STATS['factcheck_failed']}

Requ√™te originale : "{ORIGINAL_QUERY}"
"""
        
        try:
            git.commit_changes(commit_msg)
            print("‚úÖ Changements committ√©s avec succ√®s")
            logger.info("‚úÖ Changements committ√©s")
        except Exception as e:
            logger.error(f"‚ùå Erreur commit Git : {e}")
            print(f"‚ö†Ô∏è  Erreur commit Git : {e}")
    else:
        print("\n‚ö†Ô∏è  Aucun fichier cr√©√©, pas de commit Git")
    
    # ========== AFFICHAGE FINAL ==========
    print("\n" + "="*70)
    print("‚ú® EXPLORATION TERMIN√âE")
    print("="*70)
    
    if total_created > 0:
        print(f"\nüéØ R√©sultat : {total_created} nouvelles entit√©s ajout√©es √† la base")
        print(f"üìä Qualit√© : Score moyen de pertinence {sum(p.validation_score for p in validated_people)/len(validated_people):.0%}")
        print(f"üîó R√©seau : {EXPLORATION_STATS['relationships_extracted']} relations document√©es")
        print(f"‚è±Ô∏è  Dur√©e : {elapsed_time:.1f} secondes")
        print(f"\nüìÑ Rapport complet : {report_path}")
    else:
        print(f"\n‚ö†Ô∏è  Aucune nouvelle entit√© cr√©√©e")
        print(f"   Raison : Toutes les entit√©s d√©couvertes existent d√©j√†")
    
    print("\n" + "="*70)
    
    # Afficher le TOP 10 des personnes valid√©es par score
    if validated_people:
        print("\nüèÜ TOP 10 - Personnes les plus pertinentes :")
        print("="*70)
        
        top_people = sorted(validated_people, key=lambda x: x.validation_score, reverse=True)[:10]
        
        for i, person in enumerate(top_people, 1):
            status = "‚úÖ CR√â√âE" if person.name not in people_already_exist else "‚è≠Ô∏è  EXISTANTE"
            print(f"{i:2d}. {person.name}")
            print(f"    Score: {person.validation_score:.0%} | Niveau: {person.depth} | {status}")
            print(f"    Via: {person.found_via}")
            print(f"    Relations: {len(person.relationships)}")
            print()
    
    # Message final
    print("="*70)
    print("üß† ≈íil de Dieu - Mission accomplie")
    print("="*70)
    
    logger.info(f"‚úÖ Exploration termin√©e : {total_created} entit√©s cr√©√©es")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        query_arg = ' '.join(sys.argv[1:])
        main(query_arg)
    else:
        main()
